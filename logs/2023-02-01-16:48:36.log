
=============================
== Triton Inference Server ==
=============================

NVIDIA Release 22.12 (build 50109463)
Triton Server Version 2.29.0

Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

I0201 08:48:37.975790 1 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x205200000' with size 268435456
I0201 08:48:37.975960 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864
I0201 08:48:37.979662 1 model_lifecycle.cc:459] loading: action_trt:1
I0201 08:48:38.091687 1 tensorrt.cc:64] TRITONBACKEND_Initialize: tensorrt
I0201 08:48:38.091731 1 tensorrt.cc:74] Triton TRITONBACKEND API version: 1.10
I0201 08:48:38.091741 1 tensorrt.cc:80] 'tensorrt' TRITONBACKEND API version: 1.10
I0201 08:48:38.091745 1 tensorrt.cc:104] backend configuration:
{"cmdline":{"auto-complete-config":"true","min-compute-capability":"6.000000","backend-directory":"/opt/tritonserver/backends","default-max-batch-size":"4"}}
I0201 08:48:38.091973 1 tensorrt.cc:211] TRITONBACKEND_ModelInitialize: action_trt (version 1)
I0201 08:48:38.702870 1 logging.cc:49] Loaded engine size: 90 MiB
I0201 08:48:38.927893 1 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +90, now: CPU 0, GPU 90 (MiB)
I0201 08:48:38.939200 1 tensorrt.cc:260] TRITONBACKEND_ModelInstanceInitialize: action_trt_0 (GPU device 0)
I0201 08:48:38.994683 1 logging.cc:49] Loaded engine size: 90 MiB
I0201 08:48:39.062644 1 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +90, now: CPU 0, GPU 90 (MiB)
I0201 08:48:39.066934 1 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +36, now: CPU 0, GPU 126 (MiB)
W0201 08:48:39.066972 1 logging.cc:46] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars
I0201 08:48:39.067902 1 instance_state.cc:187] Created instance action_trt_0 on GPU 0 with stream priority 0 and optimization profile default[0];
I0201 08:48:39.101747 1 model_lifecycle.cc:694] successfully loaded 'action_trt' version 1
I0201 08:48:39.101921 1 server.cc:563] 
+------------------+------+
| Repository Agent | Path |
+------------------+------+
+------------------+------+

I0201 08:48:39.102011 1 server.cc:590] 
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Backend  | Path                                                      | Config                                                                                                                                                        |
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tensorrt | /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so | {"cmdline":{"auto-complete-config":"true","min-compute-capability":"6.000000","backend-directory":"/opt/tritonserver/backends","default-max-batch-size":"4"}} |
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0201 08:48:39.102054 1 server.cc:633] 
+------------+---------+--------+
| Model      | Version | Status |
+------------+---------+--------+
| action_trt | 1       | READY  |
+------------+---------+--------+

I0201 08:48:39.130748 1 metrics.cc:864] Collecting metrics for GPU 0: NVIDIA GeForce RTX 2060
I0201 08:48:39.131561 1 metrics.cc:757] Collecting CPU metrics
I0201 08:48:39.131743 1 tritonserver.cc:2264] 
+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                                |
+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | triton                                                                                                                                                                                               |
| server_version                   | 2.29.0                                                                                                                                                                                               |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace logging |
| model_repository_path[0]         | /models                                                                                                                                                                                              |
| model_control_mode               | MODE_NONE                                                                                                                                                                                            |
| strict_model_config              | 0                                                                                                                                                                                                    |
| rate_limit                       | OFF                                                                                                                                                                                                  |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                            |
| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                             |
| response_cache_byte_size         | 0                                                                                                                                                                                                    |
| min_supported_compute_capability | 6.0                                                                                                                                                                                                  |
| strict_readiness                 | 1                                                                                                                                                                                                    |
| exit_timeout                     | 30                                                                                                                                                                                                   |
+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0201 08:48:39.134370 1 grpc_server.cc:4819] Started GRPCInferenceService at 0.0.0.0:8001
I0201 08:48:39.134731 1 http_server.cc:3477] Started HTTPService at 0.0.0.0:8000
I0201 08:48:39.177641 1 http_server.cc:184] Started Metrics Service at 0.0.0.0:8002
