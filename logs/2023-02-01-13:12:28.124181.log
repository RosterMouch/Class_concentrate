
=============================
== Triton Inference Server ==
=============================

NVIDIA Release 22.12 (build 50109463)
Triton Server Version 2.29.0

Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

I0201 05:12:29.596214 1 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x205200000' with size 268435456
I0201 05:12:29.596355 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864
I0201 05:12:29.599249 1 model_lifecycle.cc:459] loading: action_trt:1
I0201 05:12:29.646685 1 tensorrt.cc:64] TRITONBACKEND_Initialize: tensorrt
I0201 05:12:29.646729 1 tensorrt.cc:74] Triton TRITONBACKEND API version: 1.10
I0201 05:12:29.646735 1 tensorrt.cc:80] 'tensorrt' TRITONBACKEND API version: 1.10
I0201 05:12:29.646739 1 tensorrt.cc:104] backend configuration:
{"cmdline":{"auto-complete-config":"true","min-compute-capability":"6.000000","backend-directory":"/opt/tritonserver/backends","default-max-batch-size":"4"}}
I0201 05:12:29.646942 1 tensorrt.cc:211] TRITONBACKEND_ModelInitialize: action_trt (version 1)
I0201 05:12:30.167901 1 logging.cc:49] Loaded engine size: 90 MiB
I0201 05:12:30.414045 1 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +90, now: CPU 0, GPU 90 (MiB)
I0201 05:12:30.432066 1 tensorrt.cc:260] TRITONBACKEND_ModelInstanceInitialize: action_trt_0 (GPU device 0)
I0201 05:12:30.522963 1 logging.cc:49] Loaded engine size: 90 MiB
I0201 05:12:30.639858 1 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +90, now: CPU 0, GPU 90 (MiB)
I0201 05:12:30.645652 1 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +36, now: CPU 0, GPU 126 (MiB)
W0201 05:12:30.645729 1 logging.cc:46] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars
I0201 05:12:30.646655 1 instance_state.cc:187] Created instance action_trt_0 on GPU 0 with stream priority 0 and optimization profile default[0];
I0201 05:12:30.680896 1 model_lifecycle.cc:694] successfully loaded 'action_trt' version 1
I0201 05:12:30.681095 1 server.cc:563] 
+------------------+------+
| Repository Agent | Path |
+------------------+------+
+------------------+------+

I0201 05:12:30.681169 1 server.cc:590] 
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Backend  | Path                                                      | Config                                                                                                                                                        |
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tensorrt | /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so | {"cmdline":{"auto-complete-config":"true","min-compute-capability":"6.000000","backend-directory":"/opt/tritonserver/backends","default-max-batch-size":"4"}} |
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0201 05:12:30.681226 1 server.cc:633] 
+------------+---------+--------+
| Model      | Version | Status |
+------------+---------+--------+
| action_trt | 1       | READY  |
+------------+---------+--------+

I0201 05:12:30.703568 1 metrics.cc:864] Collecting metrics for GPU 0: NVIDIA GeForce RTX 2060
I0201 05:12:30.704275 1 metrics.cc:757] Collecting CPU metrics
I0201 05:12:30.704589 1 tritonserver.cc:2264] 
+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                                |
+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | triton                                                                                                                                                                                               |
| server_version                   | 2.29.0                                                                                                                                                                                               |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace logging |
| model_repository_path[0]         | /models                                                                                                                                                                                              |
| model_control_mode               | MODE_NONE                                                                                                                                                                                            |
| strict_model_config              | 0                                                                                                                                                                                                    |
| rate_limit                       | OFF                                                                                                                                                                                                  |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                            |
| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                             |
| response_cache_byte_size         | 0                                                                                                                                                                                                    |
| min_supported_compute_capability | 6.0                                                                                                                                                                                                  |
| strict_readiness                 | 1                                                                                                                                                                                                    |
| exit_timeout                     | 30                                                                                                                                                                                                   |
+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0201 05:12:30.706305 1 grpc_server.cc:4819] Started GRPCInferenceService at 0.0.0.0:8001
I0201 05:12:30.706599 1 http_server.cc:3477] Started HTTPService at 0.0.0.0:8000
I0201 05:12:30.749722 1 http_server.cc:184] Started Metrics Service at 0.0.0.0:8002
